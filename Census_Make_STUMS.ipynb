{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data before fixes:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 292919 entries, 0 to 292918\n",
      "Columns: 287 entries, RT to PWGTP80\n",
      "dtypes: float64(84), int64(199), object(4)\n",
      "memory usage: 641.4+ MB\n",
      "None\n",
      "\n",
      "(FINCP, HINCP, TYPEHUGQ not in this dataset, no problem!)\n",
      "data after fixes:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 292919 entries, 0 to 292918\n",
      "Columns: 130 entries, RT to FHINS5C\n",
      "dtypes: int64(130)\n",
      "memory usage: 290.5 MB\n",
      "None\n",
      "\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# create and pickle STUMS curated subset of PUMS.\n",
    "# https://www2.census.gov/programs-surveys/acs/data/pums/2022/1-Year/downloads\n",
    "# process the original dataframe\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import socket\n",
    "\n",
    "dire = 'D:\\\\Dropbox\\\\Census\\\\Census2024\\\\FilesToGitHub\\\\'\n",
    "pfile = 'psam_ptx.csv'\n",
    "\n",
    "DOFILE = pfile\n",
    "DECILATE = True # below, replace values by deciles in selected columns\n",
    "REMAPILATE = True # below, use bespoke remappings\n",
    "\n",
    "def replace_all_by_deciles(dff, column_name): # neg and pos vals both meaningful, 1-10\n",
    "    fiddle = dff[column_name] + np.random.uniform(0, 0.01, size=len(dff))\n",
    "    decile_bins = np.percentile(fiddle, np.arange(10, 100, 10))\n",
    "    deciles = np.digitize(fiddle, bins=decile_bins, right=True) + 1\n",
    "    dff[column_name] = deciles.astype(float)\n",
    "def replace_pos_by_deciles(dff, column_name): # 1-10 leaving neg or zero unchanged\n",
    "    positive_indices = dff[column_name] > 0.\n",
    "    positive_values = dff.loc[positive_indices, column_name]\n",
    "    positive_values += np.random.uniform(0, 0.01, size=len(positive_values))\n",
    "    decile_bins = np.percentile(positive_values, np.arange(10, 100, 10))\n",
    "    deciles = np.digitize(positive_values, bins=decile_bins, right=True) + 1\n",
    "    dff.loc[positive_indices, column_name] = deciles.astype(float)\n",
    "def make_dict_of_colnams(file_path):\n",
    "    result_dict = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            words = line.split()\n",
    "            if words:  # Ensure there are words to avoid index errors\n",
    "                first_word = words[0]\n",
    "                if first_word.isupper() and ': ' in line:\n",
    "                    key, value = line.split(': ', 1)\n",
    "                    result_dict[key] = value\n",
    "    return result_dict\n",
    "def map_values(array, instructions): # example: \"-1:7, 1-4:8, 6:-1, 8-10:19\"\n",
    "    mapping_dict = {}\n",
    "    for part in instructions.split(','):\n",
    "        key, value = part.strip().split(':')\n",
    "        # Trim whitespace and convert the value to integer\n",
    "        value = float(value.strip())\n",
    "        if '-' in key and not key.startswith('-'):  # Range indicator present and not a negative number\n",
    "            start, end = map(int, key.split('-'))\n",
    "            for i in range(start, end + 1):\n",
    "                mapping_dict[i] = value\n",
    "        else:\n",
    "            mapping_dict[int(key)] = value\n",
    "    mapped_array = [mapping_dict.get(x, x) for x in array]    \n",
    "    return mapped_array\n",
    "\n",
    "\n",
    "df = pd.read_csv(dire + DOFILE)\n",
    "print('data before fixes:')\n",
    "print(df.info())\n",
    "print(\"\")\n",
    "\n",
    "PICKLEALL = False\n",
    "if PICKLEALL:\n",
    "    df.to_pickle(dire + \"psam_ptx_as_df.pkl\")\n",
    "    print(\"Pickled the full original data set.\")\n",
    "    \n",
    "SAVEWGTS = False\n",
    "if SAVEWGTS :\n",
    "    wgtcols = df.columns[df.columns.str.startswith(('PWGTP', 'WGTP'))]\n",
    "    dfwgts = df[wgtcols]\n",
    "    wgtsfile = \"PWGTPweights_df.pkl\"\n",
    "    dfwgts.to_pickle(dire + wgtsfile)\n",
    "    print(f\"Pickled {wgtsfile} with this contents:\")\n",
    "    print(dfwgts.info())\n",
    "    print(\"\")\n",
    "    \n",
    "# make a dict of long descriptions of col names\n",
    "colnamfile = dire + 'PUMS_Data_Dict_WHP_reduced.txt'\n",
    "colnamdict = make_dict_of_colnams(colnamfile)\n",
    "\n",
    "# data fixes: get rid of not now useful columns\n",
    "dff = df.loc[:, ~df.columns.str.match('^PWGTP')] # re-sample weights, separately saved\n",
    "dff = dff.loc[:, ~dff.columns.str.match('^WGTP')] # ditto\n",
    "dff = dff.loc[:, ~dff.columns.str.match('^NAICSP')] # too hard, delete\n",
    "dff = dff.loc[:, ~dff.columns.str.match('^SOCP')] # too hard, delete\n",
    "\n",
    "# get rid of allocation flags\n",
    "dropcols = [col for col in dff.columns if \"allocation flag\" in colnamdict.get(col, \"\")]\n",
    "dff = dff.drop(columns=dropcols)\n",
    "\n",
    "# RT is a special case with letters\n",
    "dff['RT'] = dff['RT'].map({'H': 1, 'P': 2}).astype(np.int64)\n",
    "# SERIALNO has letters distinguishing H from P records, not needed\n",
    "dff['SERIALNO'] = dff['SERIALNO'].str.replace('\\D', '', regex=True).astype(np.int64) # make numeric\n",
    "\n",
    "# get rid of nan's\n",
    "try: # will succeed for hfile, fail for pfile\n",
    "    dff[['FINCP', 'HINCP']] = dff[['FINCP', 'HINCP']].fillna(0) # incomes (only cols with legit neg values)\n",
    "    dff = dff[dff['TYPEHUGQ'] == 1].reset_index(drop=True) # keep only Housing Units, not Group Quarters\n",
    "    dff = dff[dff['NP'] > 0].reset_index(drop=True) # remove vacant units\n",
    "except:\n",
    "    print('(FINCP, HINCP, TYPEHUGQ not in this dataset, no problem!)')\n",
    "dff = dff.fillna(-1) # else change nans to -1\n",
    "dff = dff.astype(np.float64)\n",
    "\n",
    "if DECILATE : # reduce cardinality of answers by coding deciles on some cols\n",
    "    deccols = ['PINCP','PERNP','WAGP','SEMP','INTP',] # deciles over whole range neg and os\n",
    "    poscols = ['SSP','RETP','OIP','POVPIP','SSIP','PAP','JWMNP','WKHP',] # deciles over positives only\n",
    "    for col in deccols: \n",
    "        if col in dff: replace_all_by_deciles(dff, col)\n",
    "    for col in poscols: \n",
    "        if col in dff: replace_pos_by_deciles(dff, col)\n",
    "            \n",
    "if REMAPILATE : # reduce cardinality of answers by WHP crafterd mappings\n",
    "    bespoke = {\n",
    "        'JWAP': '1-33:1, 34-45:2, 46-57:3, 58-69:4, 70-93:5, 94-105:6, 106-117:7, 118-130:8, 131-177:9, 178-285:10',\n",
    "        'JWDP': '1-33:1, 34-45:2, 46-57:3, 58-69:4, 70-93:5, 94-105:6, 106-117:7, 118-130:8, 131-177:9, 178-285:10',\n",
    "        'LANP': '-1:1, 0:1, 1200:2, 1970-2050:3, 1000-1199:4, 1201-1969:4, 2051-9999:4',\n",
    "        'AGEP': '1-6:1, 7-12:2, 13-19:3, 20-29:4, 30-39:5, 40-49:6, 50-59:7, 60-69:8, 70-79:9, 80-99:10',\n",
    "        'YOEP': '1000-1949:1, 1950-1969:2, 1970-1989:3, 1990-1999:4, 2000-2009:5, 2010-2014:6, 2015-2019:7, 2020-9999:8',\n",
    "        'MARHYP': '1000-1949:1, 1950-1969:2, 1970-1989:3, 1990-1999:4, 2000-2009:5, 2010-2014:6, 2015-2019:7, 2020-9999:8',\n",
    "        'CITWP': '1000-1949:1, 1950-1969:2, 1970-1989:3, 1990-1999:4, 2000-2009:5, 2010-2014:6, 2015-2019:7, 2020-9999:8',\n",
    "        'WKWN': '1-9:1, 10-19:2, 20-29:3, 30-39:4, 40-49:5, 50-53:6',\n",
    "        'SCHL': '2-6:1, 7-12:2, 13-15:3, 16-17:4, 18-19:5, 20:6, 21:7, 22-23:8, 24:9',\n",
    "        'SCHG': '1-5:1, 6-11:2, 12-14:3, 15:4, 16:5',\n",
    "        'HISP': '5-12:5, 13-22:6, 23-99:7',\n",
    "        'VPS':  '1-5:1, 6-8:2, 9-10:3, 11:4, 12-99:5',\n",
    "        'JWTRNS': '3-5:3, 9:4, 10:5, 11:6, 6-8:7, 12:7',\n",
    "    }\n",
    "    for col in bespoke :\n",
    "        dff[col] = map_values(dff[col],bespoke[col])\n",
    "dff = dff.astype(np.int64) # turn it all back to integer\n",
    "            \n",
    "print('data after fixes:')\n",
    "print(dff.info()) # show size info\n",
    "print(\"\")\n",
    "\n",
    "unique_counts = dff.nunique()\n",
    "nuniquedict = unique_counts.to_dict()\n",
    "fracnandict = {col: (dff[col] < 0).sum() / dff[col].count() for col in dff.columns}\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 38 STUMS columns to start with.\n",
      "There are 233 binary STUMS columns in total.\n",
      "Created file with (292919, 38).\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# MAKE STUMS data frames:\n",
    "# subset and pickle a dataframe\n",
    "def makesmallerdf(dff, columns_list, n_rows):\n",
    "    n_rows = min(n_rows, len(dff))\n",
    "    # Randomly select rows without replacement and filter by columns\n",
    "    random_rows_df = dff.sample(n=n_rows, replace=False)[columns_list].reset_index(drop=True)    \n",
    "    return random_rows_df\n",
    "\n",
    "STUMScols = ['SEX','AGEP','SCHL','RAC1P','ESR','MAR','DIS','PINCP','NATIVITY','MIL','ENG','HICOV','FER',\n",
    "            'JWMNP','WAOB','SCH','SCHG','PERNP','SSP','PAP','MSP','MARHT','MARHYP','MARHW','MARHM','MARHD',\n",
    "            'WRK','JWAP','WKHP','WKWN','CIT','DECADE','JWTRNS','LANX','LANP','DEYE','DEAR','DREM',]\n",
    "\n",
    "print('There are',len(STUMScols),'STUMS columns to start with.')\n",
    "sum_unique_values = sum(dff[col].nunique() for col in STUMScols) # number cols in binary (not counting weights)\n",
    "print('There are',sum_unique_values,'binary STUMS columns in total.')\n",
    "\n",
    "NSAMP = 300000\n",
    "NCOLS = 100\n",
    "outfile = \"STUMS_df_all_foo.pkl\"\n",
    "\n",
    "dfff = makesmallerdf(dff,STUMScols[:NCOLS],NSAMP)\n",
    "dfff.to_pickle(dire + outfile)\n",
    "print(f\"Created file with {dfff.shape}.\")\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### ABOVE HERE and BELOW HERE are separate tasks. Above is make STUMS. Below is make STUMS-H.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "### ABOVE HERE and BELOW HERE are separate tasks. Above is make STUMS. Below is make STUMS-H."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RT, SERIALNO not in remaining dataset, no problem!)\n",
      "(FINCP, HINCP, TYPEHUGQ not in remaining dataset, no problem!)\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# create and pickle STUMS-H curated subset of PUMS.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import socket\n",
    "\n",
    "dire = 'D:\\\\Dropbox\\\\Census\\\\Census2024\\\\FilesToGitHub\\\\'\n",
    "hfile = 'psam_htx.csv'\n",
    "\n",
    "pickleallout = \"psam_htx_as_df_foo.pkl\"\n",
    "pickleweightsout = \"PWGT_H_weights_df_foo.pkl\"\n",
    "reduceddictfile = 'PUMS_Data_Dict_WHP_reduced.txt' # input for both P and H records\n",
    "PICKLEALL = False\n",
    "SAVEWGTS = False\n",
    "PRINTALL = False\n",
    "\n",
    "DOFILE = hfile\n",
    "DECILATE = True # below, replace values by deciles in selected columns\n",
    "REMAPILATE = True # below, use bespoke remappings\n",
    "\n",
    "ONLYcols = ['NP','NPF','ACR','NR','YRBLT','TEN','VALP','RMSP','HFL','WATP',\n",
    " 'BLD', 'RNTP','GRNTP','MHP','MRGP','SMP','SMOCP','INSP','TAXAMT',\n",
    " 'PLM','BATH','BDSP','KIT','VEH','TEL','ACCESSINET','SMARTPHONE',\n",
    " 'LAPTOP','BROADBND','COMPOTHX','HISPEED','HINCP','FINCP','FS','ELEP',\n",
    " 'HHT','WIF','PARTNER','HUPAC','HUGCL','CPLT','HHLDRAGEP','HHLDRRAC1P',\n",
    " 'HHL','LNGI','FPARC',]\n",
    "\n",
    "deccols = ['FINCP','HINCP',] # deciles over whole range neg and os\n",
    "poscols = ['FULP','GASP','INSP','MHP','MRGP','RNTP','SMP','VALP','WATP','GRNTP',\n",
    "          'SMOCP','TAXAMT'] # deciles over positives only leaving neg and zero unchanged\n",
    "bespoke = {\n",
    "    'NP': '7-20:7',\n",
    "    'BDSP': '6-99:6',\n",
    "    'ELEP': '2-19:2, 20-49:3, 50-99:4, 100-199:5, 200-399:6, 400-999:7, 1000-9999:8',\n",
    "    'RMSP': '4-6:4, 6-10:5, 10-99:6',\n",
    "    'YRBLT': '1000-1939:1, 1940-1959:1, 1960-1979:2, 1980-1999:3, 2000-2019:4, 2020-2099:5',\n",
    "    'HHLDRAGEP': '15-19:1, 20-29:2, 30-39:3, 40-49:4, 50-59:5, 60-69:6, 70-79:7, 80-99:8',\n",
    "    'HHLDRRAC1P': '3-5:3, 6-6:4, 7-7:3, 8-9:5',\n",
    "    'NPF': '6-8:6, 9-12:7, 13-99:8',    \n",
    "}\n",
    "\n",
    "def replace_all_by_deciles(dff, column_name): # neg and pos vals both meaningful, 1-10\n",
    "    fiddle = dff[column_name] + np.random.uniform(0, 0.01, size=len(dff))\n",
    "    decile_bins = np.percentile(fiddle, np.arange(10, 100, 10))\n",
    "    deciles = np.digitize(fiddle, bins=decile_bins, right=True) + 1\n",
    "    dff[column_name] = deciles.astype(float)\n",
    "def replace_pos_by_deciles(dff, column_name): # 1-10 leaving neg or zero unchanged\n",
    "    positive_indices = dff[column_name] > 0.\n",
    "    positive_values = dff.loc[positive_indices, column_name]\n",
    "    positive_values += np.random.uniform(0, 0.01, size=len(positive_values))\n",
    "    decile_bins = np.percentile(positive_values, np.arange(10, 100, 10))\n",
    "    deciles = np.digitize(positive_values, bins=decile_bins, right=True) + 1\n",
    "    dff.loc[positive_indices, column_name] = deciles.astype(float)\n",
    "def make_dict_of_colnams(file_path):\n",
    "    result_dict = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            words = line.split()\n",
    "            if words:  # Ensure there are words to avoid index errors\n",
    "                first_word = words[0]\n",
    "                if first_word.isupper() and ': ' in line:\n",
    "                    key, value = line.split(': ', 1)\n",
    "                    result_dict[key] = value\n",
    "    return result_dict\n",
    "def map_values(array, instructions): # example: \"-1:7, 1-4:8, 6:-1, 8-10:19\"\n",
    "    mapping_dict = {}\n",
    "    for part in instructions.split(','):\n",
    "        key, value = part.strip().split(':')\n",
    "        # Trim whitespace and convert the value to integer\n",
    "        value = float(value.strip())\n",
    "        if '-' in key and not key.startswith('-'):  # Range indicator present and not a negative number\n",
    "            start, end = map(int, key.split('-'))\n",
    "            for i in range(start, end + 1):\n",
    "                mapping_dict[i] = value\n",
    "        else:\n",
    "            mapping_dict[int(key)] = value\n",
    "    mapped_array = [mapping_dict.get(x, x) for x in array]    \n",
    "    return mapped_array\n",
    "\n",
    "\n",
    "df = pd.read_csv(dire + DOFILE)\n",
    "if PRINTALL :\n",
    "    print('data before fixes:')\n",
    "    print(df.info())\n",
    "    print(\"\")\n",
    "\n",
    "if PICKLEALL:\n",
    "    df.to_pickle(dire + pickleallout)\n",
    "    print(\"Pickled the full original data set.\")\n",
    "    \n",
    "if SAVEWGTS :\n",
    "    wgtcols = df.columns[df.columns.str.startswith(('PWGTP', 'WGTP'))]\n",
    "    dfwgts = df[wgtcols]\n",
    "    wgtsfile = pickleweightsout\n",
    "    dfwgts.to_pickle(dire + wgtsfile)\n",
    "    if PRINTALL :\n",
    "        print(f\"Pickled {wgtsfile} with this contents:\")\n",
    "        print(dfwgts.info())\n",
    "        print(\"\")\n",
    "    \n",
    "# make a dict of long descriptions of col names\n",
    "colnamfile = dire + reduceddictfile\n",
    "colnamdict = make_dict_of_colnams(colnamfile)\n",
    "\n",
    "\n",
    "# data fixes: get rid of not now useful columns\n",
    "dff = df.loc[:,ONLYcols]\n",
    "dff = dff.loc[:, ~dff.columns.str.match('^PWGTP')] # re-sample weights, separately saved\n",
    "dff = dff.loc[:, ~dff.columns.str.match('^WGTP')] # ditto\n",
    "dff = dff.loc[:, ~dff.columns.str.match('^NAICSP')] # too hard, delete\n",
    "dff = dff.loc[:, ~dff.columns.str.match('^SOCP')] # too hard, delete\n",
    "\n",
    "# get rid of allocation flags\n",
    "dropcols = [col for col in dff.columns if \"allocation flag\" in colnamdict.get(col, \"\")]\n",
    "dff = dff.drop(columns=dropcols)\n",
    "\n",
    "try:\n",
    "    # RT is a special case with letters\n",
    "    dff['RT'] = dff['RT'].map({'H': 1, 'P': 2}).astype(np.int64)\n",
    "    # SERIALNO has letters distinguishing H from P records, not needed\n",
    "    dff['SERIALNO'] = dff['SERIALNO'].str.replace('\\D', '', regex=True).astype(np.int64) # make numeric\n",
    "except :\n",
    "    print('(RT, SERIALNO not in remaining dataset, no problem!)')\n",
    "\n",
    "# get rid of nan's\n",
    "try: # will succeed for hfile, fail for pfile\n",
    "    dff[['FINCP', 'HINCP']] = dff[['FINCP', 'HINCP']].fillna(0) # incomes (only cols with legit neg values)\n",
    "    dff = dff[dff['TYPEHUGQ'] == 1].reset_index(drop=True) # keep only Housing Units, not Group Quarters\n",
    "    dff = dff[dff['NP'] > 0].reset_index(drop=True) # remove vacant units\n",
    "except:\n",
    "    print('(FINCP, HINCP, TYPEHUGQ not in remaining dataset, no problem!)')\n",
    "dff = dff.fillna(-1) # else change nans to -1\n",
    "dff = dff.astype(np.float64)\n",
    "\n",
    "if DECILATE : # reduce cardinality of answers by coding deciles on some cols\n",
    "    for col in deccols: \n",
    "        if col in dff: replace_all_by_deciles(dff, col)\n",
    "    for col in poscols: \n",
    "        if col in dff: replace_pos_by_deciles(dff, col)\n",
    "            \n",
    "if REMAPILATE : # reduce cardinality of answers by WHP crafterd mappings\n",
    "    for col in bespoke :\n",
    "        dff[col] = map_values(dff[col],bespoke[col])\n",
    "dff = dff.astype(np.int64) # turn it all back to integer\n",
    "            \n",
    "if PRINTALL :\n",
    "    print('data after fixes:')\n",
    "    print(dff.info()) # show size info\n",
    "    print(\"\")\n",
    "\n",
    "unique_counts = dff.nunique()\n",
    "nuniquedict = unique_counts.to_dict()\n",
    "fracnandict = {col: (dff[col] < 0).sum() / dff[col].count() for col in dff.columns}\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 46 STUMS-H columns to start with.\n",
      "There are 315 binary STUMS-H columns in total.\n",
      "Created file STUMS-H_df_all_foo.pkl with (133016, 46).\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# MAKE STUMS-H data frames:\n",
    "# subset and pickle a dataframe\n",
    "def makesmallerdf(dff, columns_list, n_rows):\n",
    "    n_rows = min(n_rows, len(dff))\n",
    "    # Randomly select rows without replacement and filter by columns\n",
    "    if n_rows == len(dff) :\n",
    "        random_rows_df = dff\n",
    "    else :\n",
    "        random_rows_df = dff.sample(n=n_rows, replace=False)[columns_list].reset_index(drop=True)    \n",
    "    return random_rows_df\n",
    "\n",
    "STUMSHcols = ONLYcols\n",
    "print('There are',len(STUMSHcols),'STUMS-H columns to start with.')\n",
    "sum_unique_values = sum(dff[col].nunique() for col in STUMSHcols) # number cols in binary (not counting weights)\n",
    "print('There are',sum_unique_values,'binary STUMS-H columns in total.')\n",
    "\n",
    "outfile = \"STUMS-H_df_all_foo.pkl\"\n",
    "\n",
    "dfff =  dff #makesmallerdf(dff,STUMScols[:NCOLS],NSAMP)\n",
    "dfff.to_pickle(dire + outfile)\n",
    "print(f\"Created file {outfile} with {dfff.shape}.\")\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-time get reduced version of data dictionary\n",
    "# this, with some hand-editing, produces the file PUMS_Data_Dict_WHP_reduced.txt\n",
    "dire = 'D:\\\\Dropbox\\\\Census\\\\Census2024\\\\FilesToGitHub\\\\'\n",
    "filnam = 'PUMS_Data_Dictionary_2022.txt'\n",
    "\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()  # Read all lines into a list\n",
    "\n",
    "    for i in range(len(lines) - 1):  # Loop through lines, stopping at the second to last to avoid index error\n",
    "        words = lines[i].split()  # Split the current line into words\n",
    "        if words:  # Check if the line is not empty\n",
    "            first_word = words[0]  # Get the first word\n",
    "            if first_word.isupper():  # Check if the first word is in all uppercase\n",
    "                output_line = f\"{first_word}: {lines[i+1].strip()}\"  # Prepare the output line\n",
    "                print(output_line) # or else change to save to a file\n",
    "\n",
    "if False : # change to True to run\n",
    "    process_file(dire+filnam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
